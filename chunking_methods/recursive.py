from typing import Generator
from collections import defaultdict, deque
import re
import os
import logging
from typing import List
import os
import uuid
import spacy

class ChunkSplitter2:
    """Custom MarkdownTextSplitter for splitting documents into chunks."""

    def __init__(self, chunk_size: int, chunk_overlap: int, metadata_in_chunk: bool, nlp_backend="spacy"):
        self.headers_to_split_on = sorted(
            [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")],
            key=lambda header: len(header[0]),
            reverse=True,
        )
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.metadata_in_chunk = metadata_in_chunk
        if nlp_backend == "kiwi":
            from kiwi import Kiwi
            self.sentence_splitter = Kiwi()
        else:
            self.sentence_splitter = spacy.load("en_core_web_sm")
        
        self.recursive_separators = ["split_by_sent", "?", "!", "#", ";", ":", ".", " "]
        # self.recursive_separators = ["\n", "?", "!", ",", ";", ":", ".", " "]

        self.last_seperator = self.recursive_separators[-1]
        self.contents_in_metadata = defaultdict(deque)
        self.line_lengths_in_metadata = defaultdict(deque)
        self.contents_len_in_metadata = defaultdict(int)
        self.header_stack = []

        self.chunk_idx = 0
        # self.result_wrapper = {'result': True,
        #                        'status': 'COMPLETED',
        #                        'statusMessage': 'CHUNK_SPLIT_SUCCESS'}
        self.total_chunks = []

    def __call__(self, text: str, source: str, file_ext: str, file_id: str):
        file_title = os.path.splitext(os.path.split(source)[-1])[0]
        self.file_header = f"[metadata]\n문서 파일: {file_title}\n"
        self.file_ext = file_ext
        if self.file_ext == ".md":
            self.recursive_separators.insert(0, "split_by_header")

        self.file_id = file_id
        self.source = source

        # self.result_wrapper.update({"fileId":file_id, "path":source})
        return self._split_text_recursively(text, "", "", self.recursive_separators, True)

    def _split_text_recursively(self,
                                text: str,
                                meta_key: str,
                                metadata: str,
                                separators: list,
                                is_start: bool = False) -> Generator[str, None, None]:
        """Split text recursively using recursive separators and yield chunks."""
        if separators:
            current_separator = separators[0]
            for doc_piece in self._split_by_separators(current_separator, text):
                doc_piece = re.sub(r'[\x00-\x09\x0B-\x1F]+', '', doc_piece) if is_start else doc_piece

                if is_start:
                    """Update metadata"""
                    if self.file_ext == '.md' and self._is_header_line(doc_piece, self.header_stack):
                        meta_key = "\n".join(self.header_stack)
                        """If MD file, update metadata with the current header stack."""
                        metadata = f"{self.file_header}\n{meta_key}\n\n[content]\n" if self.metadata_in_chunk else metadata
                        continue
                    elif self.file_ext == '.txt':
                        """If TXT file, update metadata with only the file title."""
                        metadata = f"{self.file_header}\n\n[content]\n" if self.metadata_in_chunk else metadata

                if self._is_skip_candidate(doc_piece, meta_key):
                    continue

                doc_piece_length = len(doc_piece)
                total_length = len(metadata) + self.contents_len_in_metadata[meta_key] + doc_piece_length
                if total_length > self.chunk_size:
                    if current_separator == self.last_seperator:
                        self._result_wrapper(meta_key, "".join(self.contents_in_metadata[meta_key]).rstrip())
                        # result = self._result_wrapper(meta_key, "".join(self.contents_in_metadata[meta_key]).rstrip())
                        # if result is not None:
                        #     yield result

                        # Remove contents from the beginning until <= chunk_overlap
                        while self.contents_len_in_metadata[meta_key] > self.chunk_overlap:
                            self.contents_in_metadata[meta_key].popleft()
                            self.contents_len_in_metadata[meta_key] -= self.line_lengths_in_metadata[meta_key].popleft()
                    else:
                        # Recursive split if piece exceeds chunk size
                        # "yield from" is used to deliver the values generated by a sub-generator to the main generator.
                        for i, next_separator in enumerate(separators[1:]):
                            if next_separator == 'split_by_sent' \
                                    or re.search(re.escape(next_separator), doc_piece) is not None:
                                yield from self._split_text_recursively(doc_piece, meta_key, metadata, separators[i+1:])
                                break
                        continue

                # Append the next doc piece
                self.contents_in_metadata[meta_key].append(doc_piece)
                self.line_lengths_in_metadata[meta_key].append(doc_piece_length)
                self.contents_len_in_metadata[meta_key] += doc_piece_length

        if is_start:
            # yield remaining contents
            for meta_key, contents in self.contents_in_metadata.items():
                self._result_wrapper(meta_key, "".join(contents).rstrip())
                # result = self._result_wrapper(meta_key, "".join(contents).rstrip())
                # if result is not None:
                #     yield result

            for res in self.yield_final_res():
                yield res


    def _is_header_line(self, line: str, header_stack: list) -> bool:
        """Check if the line is a header and update metadata. (MD only)"""
        for sep, name in self.headers_to_split_on:
            if line.startswith(sep) and (len(line) == len(sep) or line[len(sep)] == " "):
                current_header_level = sep.count("#")
                while header_stack and int(header_stack[-1][7]) >= current_header_level:
                    header_stack.pop()
                header = f"Header {current_header_level}: {line[len(sep):].strip()}"
                header_stack.append(header)
                return True
        return False

    def _split_by_separators(self, separator, text):
        if separator == "split_by_header":
            pattern = r'(^#{1,}\s.*?(?:\n(?!^#{1,}\s).+)*?)\n|(^((?!^#{1,}\s).|\n)+?)(?=^#{1,}\s|\Z)'
            """
            ^#{1,}\s.*?            ==> 헤더 줄 (# + 공백 으로 시작)
            (?:\n(?!^#{1,}\s).+)*? ==> 그 뒤로 헤더가 아닌 줄 계속 붙임 (lazy)
            \n                     ==> 줄바꿈 나오면 헤더 청크 끝
            ^((?!^#{1,}\s).|\n)+?  ==> 헤더 아닌 본문 → 다음 헤더 나올 때까지 계속 붙임
            (?=^#{1,}\s|\Z)        ==> 다음 헤더 or EOF 만나면 본문 청크 끝
            """

            matches = re.finditer(pattern, text, flags=re.MULTILINE)
            """
            정규표현식의 ^ 나 $ 같은 "줄의 시작" or "줄의 끝" 조건이 있고,
            한 줄(line)의 시작에서 헤더가 나오는 경우이기 때문에 
            반드시 re.MULTILINE 있어야 제대로 동작함.
            """
            for match in matches:
                chunk = match.group(0).strip()
                if chunk:
                    yield chunk

        elif separator == "split_by_sent":
            doc = self.sentence_splitter(text)
            for txt in doc.sents:
                txt = txt.text
                txt = txt.replace('\n', ' ')
                txt = re.sub(r'\s{2,}', ' ', txt)
                yield txt.strip() + " "

        else:
            # a bit slower than str.split
            # Escape the separator if necessary (for special regex characters)
            escaped_sep = re.escape(separator)

            # Regex pattern to split and include the separator
            pattern = f"{escaped_sep}|[^{escaped_sep}]+{escaped_sep}|[^{escaped_sep}]+"

            # Apply the pattern to split the text
            split_texts = re.finditer(pattern, text)
            for split_text in split_texts:
                chunk = split_text.group(0)
                if chunk:
                    yield chunk

    def _is_skip_candidate(self, text, meta_key):
        """ chunk pieces that should be excluded """
        # if multiple '\n', then exclude from chunk"""
        if text == "\n"\
                and meta_key in self.contents_in_metadata\
                and self.contents_in_metadata[meta_key][-1][-1] == text:
            return True
        if not text:
            return True
        return False

    def _result_wrapper(self, meta_key, chunk):
        if chunk:
            metadata = "".join([self.file_header, meta_key, "\n\n[content]\n"]) if self.metadata_in_chunk else ""
            # self.result_wrapper.update({"chunk": "".join([metadata, chunk]).strip(),
            #                             "primaryKey": f"{self.file_id}_{self.chunk_idx}"})

            res = {"result" : True,
                   "status" : "COMPLETED",
                   "statusMessage" : "CHUNK_SPLIT_SUCCESS",
                   "chunk" : "".join([metadata, chunk]).strip(),
                   "primaryKey": f"{self.file_id}_{self.chunk_idx}",
                   "fileId": self.file_id,
                   "path" : self.source}
            self.total_chunks.append(res)
            self.chunk_idx += 1

        #     return self.result_wrapper
        # return None

    def yield_final_res(self):
        num_total_chunks = len(self.total_chunks)
        for res in self.total_chunks:
            res['chunkCnt'] = num_total_chunks
            yield res

class FixedLenChunker:
    """고정 길이 기반으로 텍스트를 청크로 분할"""
    
    def __init__(self, embeddings: None, nlp_backend="spacy"):
        self.embeddings = embeddings  # 다른 청킹 메서드와의 일관성을 위해 추가 (사용하지 않음)
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(logging.INFO)
        
        # BEIR 데이터셋 도메인 매핑
        self.domain_mapping = {
            'nfcorpus': 'nfcorpus',
            'scifact': 'scifact', 
            'fiqa': 'fiqa',
            'arguana': 'arguana',
            'scidocs': 'scidocs',
        }
                
        self.process_counter = 0
        self.nlp_backend = nlp_backend
    
    def split_text(self, text: str, source: str, file_name: str) -> List[str]:
        """텍스트를 고정 길이 기반으로 청크 분할"""
        if source not in self.domain_mapping:
            raise ValueError(f"Source must be one of: {list(self.domain_mapping.keys())}")
        
        canonical_source = self.domain_mapping[source]
        self.process_counter += 1
        unique_id = self.generate_unique_id()
        
        try:
            self.logger.info(f"Starting FixedLenChunker.split_text for {canonical_source} document: {unique_id}")
            
            # 매번 새로운 ChunkSplitter2 인스턴스 생성하여 상태 초기화
            chunker = ChunkSplitter2(
                chunk_size=1000, 
                chunk_overlap=50, 
                metadata_in_chunk=False,
                nlp_backend=self.nlp_backend
            )
            chunk_texts= [s['chunk'] for s in chunker(text, file_name, source, unique_id)]
            
            self.logger.info(f"FixedLenChunker created {len(chunk_texts)} chunks for {canonical_source} document: {unique_id}")
            return chunk_texts
        except Exception as e:
            self.logger.error(f"Error in FixedLenChunker.split_text for {canonical_source} document {self.process_counter} ({file_name}): {e}")
            print(f"An error occurred: {e}")
            print("Returning original text as a single chunk.")
            return [text]

    def generate_unique_id(self) -> str:
        return str(uuid.uuid4())[:8]  # Use first 8 characters of a UUID